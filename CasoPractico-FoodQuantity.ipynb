{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso Práctico  - Pamela Arico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Frases de ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para iniciar con el caso práctico es necesario construir un corpus que contenga una variedad de frases que permitan generar una gramática lo más general posible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('food_orders.txt', 'w') as f: \n",
    "    f.write('''Quiero una pizza con jamón.\n",
    "    Quiero una pizza con queso y jamón.\n",
    "    Quiero 4 hamburguesas con tocino.\n",
    "    Quiero una tortilla y una cerveza.\n",
    "    Me pones un pollo y una ensalada.\n",
    "    Quiero una paella.\n",
    "    Quiero un bocadillo.\n",
    "    Quiero una pizza.\n",
    "    Ponme 3 sopas.\n",
    "    Quiero un filete.\n",
    "    Por favor, me das tres tacos.\n",
    "    Me puedes ayudar con una orden de papitas y camotes fritos.\n",
    "    Ayudeme con cuatro hamburguesas con cebollas, mayonessa y mostaza.\n",
    "    Por favor dos hotdogs con salsa de piña.\n",
    "    Quisiera 10 alitas bbq.\n",
    "    Por favor, me puedes ayudar con una ensalada con pollo.\n",
    "    Seis bolones con queso.\n",
    "    Quiero 12 tortillas.\n",
    "    Quiero una Lasagna.\n",
    "    Quiero 3 bocadillos de anchoas y 2 pizzas.\n",
    "    Quisiera pedir una hamburguesa''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def load_corpus(txt_file):\n",
    "    with open(txt_file) as f: \n",
    "        data = str(f.read())\n",
    "    sentences = sent_tokenize(data)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quiero una pizza con jamón.',\n",
       " 'Quiero una pizza con queso y jamón.',\n",
       " 'Quiero 4 hamburguesas con tocino.',\n",
       " 'Quiero una tortilla y una cerveza.',\n",
       " 'Me pones un pollo y una ensalada.',\n",
       " 'Quiero una paella.',\n",
       " 'Quiero un bocadillo.',\n",
       " 'Quiero una pizza.',\n",
       " 'Ponme 3 sopas.',\n",
       " 'Quiero un filete.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "food_orders = load_corpus('food_orders.txt')\n",
    "food_orders[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tagger en español"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tagger con NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso consiste en entrenar un tagger en español. Para ello se va a utilizar el corpus **cess_esp** disponible en la librería de nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import cess_esp as cess\n",
    "from nltk import UnigramTagger, BigramTagger, TrigramTagger, DefaultTagger\n",
    "from nltk.tag.hmm import HiddenMarkovModelTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cess_sents = cess.tagged_sents()        # Obtengo las oraciones ya etiquetadas del corpus\n",
    "train = int(len(cess_sents)*90/100)     # Obtengo los indices para separar el set de entrenamiento / prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se van a entrenar distintos tipos de taggers para posteriormente seleccionar el que mayor presición presente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_tagger = DefaultTagger('ncms000')\n",
    "unigram_tagger = UnigramTagger(cess_sents[:train], backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(cess_sents[:train], backoff=unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(cess_sents[:train], backoff=bigram_tagger)\n",
    "hmm_tagger = HiddenMarkovModelTagger.train(cess_sents[:train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Tagger: 0.0638236977676016\n",
      "Unigram Tagger: 0.8218374356038924\n",
      "Bigram Tagger: 0.8346451058958214\n",
      "Trigram Tagger: 0.8341442472810532\n",
      "HMM Tagger: 0.8490269032627361\n"
     ]
    }
   ],
   "source": [
    "print(f'Default Tagger: {default_tagger.evaluate(cess_sents[train:])}')\n",
    "print(f'Unigram Tagger: {unigram_tagger.evaluate(cess_sents[train:])}')\n",
    "print(f'Bigram Tagger: {bigram_tagger.evaluate(cess_sents[train:])}')\n",
    "print(f'Trigram Tagger: {trigram_tagger.evaluate(cess_sents[train:])}')\n",
    "print(f'HMM Tagger: {hmm_tagger.evaluate(cess_sents[train:])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En los resultados se puede observar que el tagger con mayor presición es el **hmm tagger**. Por lo tanto, se va a realizar una prmera prueba con una de las oraciones del corpus de ordenes de comida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Quiero', 'una', 'pizza', 'con', 'jamón', '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(food_orders[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Quiero', 'sps00'),\n",
       " ('una', 'di0fs0'),\n",
       " ('pizza', 'ncfs000'),\n",
       " ('con', 'sps00'),\n",
       " ('jamón', 'ncms000'),\n",
       " ('.', 'Fp')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_tagger.tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regex parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import *\n",
    "from nltk.chunk.util import *\n",
    "from nltk.chunk.regexp import *\n",
    "from nltk import Tree\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se realiza una prueba para verificar los tags que se asignan a cada una de las palabras y así poder construir la gramática correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Quiero', 'sps00'), ('una', 'di0fs0'), ('pizza', 'ncfs000'), ('con', 'sps00'), ('jamón', 'ncms000'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('una', 'di0fs0'), ('pizza', 'ncfs000'), ('con', 'sps00'), ('queso', 'np0000l'), ('y', 'cc'), ('jamón', 'ncms000'), ('.', 'Fp')]\n",
      "[('Quiero', 'da0mp0'), ('4', 'Z'), ('hamburguesas', 'ncmp000'), ('con', 'sps00'), ('tocino', 'np0000l'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('una', 'di0fs0'), ('tortilla', 'ncfs000'), ('y', 'cc'), ('una', 'di0fs0'), ('cerveza', 'ncfs000'), ('.', 'Fp')]\n",
      "[('Me', 'pp1cs000'), ('pones', 'vmip3s0'), ('un', 'di0ms0'), ('pollo', 'ncms000'), ('y', 'cc'), ('una', 'di0fs0'), ('ensalada', 'ncfs000'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('una', 'di0fs0'), ('paella', 'ncfs000'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('un', 'di0ms0'), ('bocadillo', 'ncms000'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('una', 'di0fs0'), ('pizza', 'ncfs000'), ('.', 'Fp')]\n",
      "[('Ponme', 'da0mp0'), ('3', 'Z'), ('sopas', 'ncfp000'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('un', 'di0ms0'), ('filete', 'ncms000'), ('.', 'Fp')]\n",
      "[('Por', 'sps00'), ('favor', 'ncms000'), (',', 'Fc'), ('me', 'pp1cs000'), ('das', 'vmip3s0'), ('tres', 'dn0cp0'), ('tacos', 'ncmp000'), ('.', 'Fp')]\n",
      "[('Me', 'pp1cs000'), ('puedes', 'vmip3s0'), ('ayudar', 'vmn0000'), ('con', 'sps00'), ('una', 'di0fs0'), ('orden', 'ncfs000'), ('de', 'sps00'), ('papitas', 'np0000l'), ('y', 'cc'), ('camotes', 'ncmp000'), ('fritos', 'aq0mpp'), ('.', 'Fp')]\n",
      "[('Ayudeme', 'rg'), ('con', 'sps00'), ('cuatro', 'dn0cp0'), ('hamburguesas', 'ncmp000'), ('con', 'sps00'), ('cebollas', 'np0000l'), (',', 'Fc'), ('mayonessa', 'np0000p'), ('y', 'cc'), ('mostaza', 'ncfs000'), ('.', 'Fp')]\n",
      "[('Por', 'sps00'), ('favor', 'da0mp0'), ('dos', 'dn0cp0'), ('hotdogs', 'ncmp000'), ('con', 'sps00'), ('salsa', 'vmn0000'), ('de', 'sps00'), ('piña', 'np0000l'), ('.', 'Fp')]\n",
      "[('Quisiera', 'da0mp0'), ('10', 'Z'), ('alitas', 'ncmp000'), ('bbq', 'aq0mp0'), ('.', 'Fp')]\n",
      "[('Por', 'sps00'), ('favor', 'ncms000'), (',', 'Fc'), ('me', 'pp1cs000'), ('puedes', 'vmip3s0'), ('ayudar', 'vmn0000'), ('con', 'sps00'), ('una', 'di0fs0'), ('ensalada', 'ncfs000'), ('con', 'sps00'), ('pollo', 'ncms000'), ('.', 'Fp')]\n",
      "[('Seis', 'da0ms0'), ('bolones', 'ncms000'), ('con', 'sps00'), ('queso', 'np0000l'), ('.', 'Fp')]\n",
      "[('Quiero', 'da0mp0'), ('12', 'Z'), ('tortillas', 'ncfp000'), ('.', 'Fp')]\n",
      "[('Quiero', 'sps00'), ('una', 'di0fs0'), ('Lasagna', 'ncfs000'), ('.', 'Fp')]\n",
      "[('Quiero', 'da0mp0'), ('3', 'Z'), ('bocadillos', 'ncmp000'), ('de', 'sps00'), ('anchoas', 'np0000l'), ('y', 'cc'), ('2', 'Z'), ('pizzas', 'Fpt'), ('.', 'Fp')]\n",
      "[('Quisiera', 'sps00'), ('pedir', 'vmn0000'), ('una', 'di0fs0'), ('hamburguesa', 'ncfs000')]\n"
     ]
    }
   ],
   "source": [
    "for order in food_orders:\n",
    "    tokens = nltk.word_tokenize(order)      # se separa en tokens cada orden\n",
    "    tagged = hmm_tagger.tag(tokens)         # se asigna el pos tag correspondiente a cada token\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda se define la gramática que permitirá extraer la comida y la cantidad de cada orden. Se ha podido identificar los siguientes patrones a partir de los tags de la celda anterior:\n",
    "* Las cantidades cuando están escritas como palablas se identifican como determinante numérico (tag: dn...)\n",
    "* Cuando las cantidades vienen dadas por números el tag es Z\n",
    "* Por lo general los sustantivos de la comida vienen precedidos por un determinante indefinido (uno, un, una)\n",
    "* Los ingredientes, que serán tomados en cuenta como parte de la comida, vienen precedidos de una preposición simple\n",
    "* En caso de no cumplir ninguna de estas condiciones, los sustantivos que se encuentren solos serán clasificados como comida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gramática utilizada para extraer la comida y las cantidades\n",
    "gramar = '''Q:           {<Z>|<dn.*>}                                                           # Q->cantidades \n",
    "            Food:        {(<di0..0> <n.*>|(<sps00><n.*>((<cc>|<Fc>)(<sn.*>|<n.*>))*)|<n.*>)}\n",
    "         '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza una prueba de la gramática con una de las ordenes de comida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Por', 'sps00'), ('favor', 'ncms000'), (',', 'Fc'), ('me', 'pp1cs000'), ('das', 'vmip3s0'), ('tres', 'dn0cp0'), ('tacos', 'ncmp000'), ('.', 'Fp')]\n",
      "(S\n",
      "  (Food Por/sps00 favor/ncms000)\n",
      "  ,/Fc\n",
      "  me/pp1cs000\n",
      "  das/vmip3s0\n",
      "  (Q tres/dn0cp0)\n",
      "  (Food tacos/ncmp000)\n",
      "  ./Fp)\n",
      "(Food Por/sps00 favor/ncms000)\n",
      "(Food tacos/ncmp000)\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(food_orders[10])\n",
    "tagged = hmm_tagger.tag(tokens)\n",
    "print(tagged)\n",
    "chunkParser = nltk.RegexpParser(gramar)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Food'):\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el resultado de la prueba se puede observar que se identifica de forma correcta a **tacos** como comida. Sin embargo, también se ha etiquetado incorrectamente a **por favor** como comida.\n",
    "Para solventar este error se va a incluir un chinker, que retire la secuencia de preposición-sustantivo del chunker cuando venga seguida de un sigo de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gramática utilizada para extraer la comida y las cantidades\n",
    "gramar = '''\n",
    "            Q:           {<Z>|<dn.*>|<di0.s0>}                                              # Q->cantidades \n",
    "            Food:        {(<sps00><n.*><Fc>*((<cc>|<Fc>)(<sn.*>|<n.*>))*)|<n.*>}            # Chunker de comidas\n",
    "                         }<sps00><ncms000><Fc>{                                             # Chinker para retirar por favor\n",
    "         '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Por', 'sps00'), ('favor', 'ncms000'), (',', 'Fc'), ('me', 'pp1cs000'), ('das', 'vmip3s0'), ('tres', 'dn0cp0'), ('tacos', 'ncmp000'), ('.', 'Fp')]\n",
      "(S\n",
      "  Por/sps00\n",
      "  favor/ncms000\n",
      "  ,/Fc\n",
      "  me/pp1cs000\n",
      "  das/vmip3s0\n",
      "  (Q tres/dn0cp0)\n",
      "  (Food tacos/ncmp000)\n",
      "  ./Fp)\n",
      "(Food tacos/ncmp000)\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(food_orders[10])\n",
    "tagged = hmm_tagger.tag(tokens)\n",
    "print(tagged)\n",
    "chunkParser = nltk.RegexpParser(gramar)\n",
    "chunked = chunkParser.parse(tagged)\n",
    "print(chunked)\n",
    "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Food'):\n",
    "    print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se definen varias funciones encargadas de tareas específicas dentro del procesamiento de las ordenes de comida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(sentence):\n",
    "    '''Función que obtiene los chunks definidos por la gramática'''\n",
    "    tokens = nltk.word_tokenize(sentence.lower())\n",
    "    tagged = hmm_tagger.tag(tokens)\n",
    "    chunked = chunkParser.parse(tagged)\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_order_details(chunked, tag):\n",
    "    '''Función que obtiene el valor según el tag de entrada'''\n",
    "    chunk = [subtree.leaves() for subtree in chunked.subtrees(filter=lambda t: t.label() == tag)]\n",
    "    #chunked.draw()\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_num(q):\n",
    "    '''Función que convierte las palabras que representan cantidades en números'''\n",
    "    num = 1\n",
    "    word_num = q[0][0][0] if q else 1\n",
    "    if word_num == 'uno' or word_num == 'una': num = 1\n",
    "    elif word_num == 'dos': num = 2\n",
    "    elif word_num == 'tres': num = 3\n",
    "    elif word_num == 'cuatro': num = 4\n",
    "    elif word_num == 'cinco': num = 5\n",
    "    elif word_num == 'seis': num = 6\n",
    "    elif word_num == 'siete': num = 7\n",
    "    elif word_num == 'ocho': num = 8\n",
    "    elif word_num == 'nueve': num = 9\n",
    "    elif word_num == 'diez': num = 10\n",
    "    else:\n",
    "        try:\n",
    "            num = int(word_num)\n",
    "        except:\n",
    "            num = 1\n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food(chunked):\n",
    "    '''Función que extrae los sustantivos del chunk de comida'''\n",
    "    food = []\n",
    "    food_raw = get_order_details(chunked, 'Food')\n",
    "    if food_raw:\n",
    "        for item in food_raw:\n",
    "            for word in item:\n",
    "                if re.search('(^n|sn.e-SUJ)',word[1]):\n",
    "                    food.append(word[0])\n",
    "    return food"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_order_dict(food, q):\n",
    "    '''Función que construye un diccionario con la información de la orden'''\n",
    "    order = {'comida' : food,\n",
    "             'cantidad' : q\n",
    "            }\n",
    "    return order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_order_info(order, food, quantity):\n",
    "    '''Función usada para imprimir de manera ordenada los detalles de la orden'''\n",
    "    print('-------------------------------------')\n",
    "    print(f'ORDER: {order}')\n",
    "    print(f'FOOD:        {food}')\n",
    "    print(f'QUANTITY:    {quantity}')\n",
    "    print('-------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a utilizar las funciones ya definidas para ir porcesando una a una las ordenes de comida.\n",
    "Además, se va a llenar dos listas. La primera va a almacenar los árboles con los IOB tags; y la segunda, los diccionarios con la información de comida y cantidad de cada orden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "ORDER: Quiero una pizza con jamón.\n",
      "FOOD:        ['pizza', 'jamón']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero una pizza con queso y jamón.\n",
      "FOOD:        ['pizza', 'queso', 'jamón']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero 4 hamburguesas con tocino.\n",
      "FOOD:        ['hamburguesas', 'tocino']\n",
      "QUANTITY:    4\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero una tortilla y una cerveza.\n",
      "FOOD:        ['tortilla', 'cerveza']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Me pones un pollo y una ensalada.\n",
      "FOOD:        ['pollo', 'ensalada']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero una paella.\n",
      "FOOD:        ['paella']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero un bocadillo.\n",
      "FOOD:        ['bocadillo']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero una pizza.\n",
      "FOOD:        ['pizza']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Ponme 3 sopas.\n",
      "FOOD:        ['sopas']\n",
      "QUANTITY:    3\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero un filete.\n",
      "FOOD:        ['filete']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Por favor, me das tres tacos.\n",
      "FOOD:        ['tacos']\n",
      "QUANTITY:    3\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Me puedes ayudar con una orden de papitas y camotes fritos.\n",
      "FOOD:        ['orden', 'papitas', 'camotes']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Ayudeme con cuatro hamburguesas con cebollas, mayonessa y mostaza.\n",
      "FOOD:        ['hamburguesas', 'cebollas', 'mayonessa', 'mostaza']\n",
      "QUANTITY:    4\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Por favor dos hotdogs con salsa de piña.\n",
      "FOOD:        ['hotdogs', 'piña']\n",
      "QUANTITY:    2\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quisiera 10 alitas bbq.\n",
      "FOOD:        ['alitas']\n",
      "QUANTITY:    10\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Por favor, me puedes ayudar con una ensalada con pollo.\n",
      "FOOD:        ['ensalada', 'pollo']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Seis bolones con queso.\n",
      "FOOD:        ['bolones', 'queso']\n",
      "QUANTITY:    6\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero 12 tortillas.\n",
      "FOOD:        ['tortillas']\n",
      "QUANTITY:    12\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero una Lasagna.\n",
      "FOOD:        ['lasagna']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quiero 3 bocadillos de anchoas y 2 pizzas.\n",
      "FOOD:        ['bocadillos', 'anchoas']\n",
      "QUANTITY:    3\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "ORDER: Quisiera pedir una hamburguesa\n",
      "FOOD:        ['hamburguesa']\n",
      "QUANTITY:    1\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chunkParser = nltk.RegexpParser(gramar)\n",
    "iob_tagged = []\n",
    "orders_info = []\n",
    "for order in food_orders:\n",
    "    chunked = get_chunks(order)\n",
    "    #print(chunked)\n",
    "    iob_tagged.append(chunked)\n",
    "    food = get_food(chunked)\n",
    "    q = get_order_details(chunked, 'Q')\n",
    "    n = map_num(q)\n",
    "    orders_info.append(build_order_dict(food, n))\n",
    "    print_order_info(order, food, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'comida': ['pizza', 'jamón'], 'cantidad': 1},\n",
       " {'comida': ['pizza', 'queso', 'jamón'], 'cantidad': 1},\n",
       " {'comida': ['hamburguesas', 'tocino'], 'cantidad': 4},\n",
       " {'comida': ['tortilla', 'cerveza'], 'cantidad': 1},\n",
       " {'comida': ['pollo', 'ensalada'], 'cantidad': 1},\n",
       " {'comida': ['paella'], 'cantidad': 1},\n",
       " {'comida': ['bocadillo'], 'cantidad': 1},\n",
       " {'comida': ['pizza'], 'cantidad': 1},\n",
       " {'comida': ['sopas'], 'cantidad': 3},\n",
       " {'comida': ['filete'], 'cantidad': 1},\n",
       " {'comida': ['tacos'], 'cantidad': 3},\n",
       " {'comida': ['orden', 'papitas', 'camotes'], 'cantidad': 1},\n",
       " {'comida': ['hamburguesas', 'cebollas', 'mayonessa', 'mostaza'],\n",
       "  'cantidad': 4},\n",
       " {'comida': ['hotdogs', 'piña'], 'cantidad': 2},\n",
       " {'comida': ['alitas'], 'cantidad': 10},\n",
       " {'comida': ['ensalada', 'pollo'], 'cantidad': 1},\n",
       " {'comida': ['bolones', 'queso'], 'cantidad': 6},\n",
       " {'comida': ['tortillas'], 'cantidad': 12},\n",
       " {'comida': ['lasagna'], 'cantidad': 1},\n",
       " {'comida': ['bocadillos', 'anchoas'], 'cantidad': 3},\n",
       " {'comida': ['hamburguesa'], 'cantidad': 1}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  quiero/sps00\n",
      "  (Q una/di0fs0)\n",
      "  (Food pizza/ncfs000)\n",
      "  (Food con/sps00 jamón/ncms000)\n",
      "  ./Fp)\n",
      "(S\n",
      "  quiero/sps00\n",
      "  (Q una/di0fs0)\n",
      "  (Food pizza/ncfs000)\n",
      "  (Food con/sps00 queso/np0000l y/cc jamón/ncms000)\n",
      "  ./Fp)\n",
      "(S\n",
      "  quiero/da0mp0\n",
      "  (Q 4/Z)\n",
      "  (Food hamburguesas/ncmp000)\n",
      "  (Food con/sps00 tocino/np0000l)\n",
      "  ./Fp)\n",
      "(S\n",
      "  quiero/sps00\n",
      "  (Q una/di0fs0)\n",
      "  (Food tortilla/ncfs000)\n",
      "  y/cc\n",
      "  (Q una/di0fs0)\n",
      "  (Food cerveza/ncfs000)\n",
      "  ./Fp)\n",
      "(S\n",
      "  me/pp1cs000\n",
      "  pones/vmip3s0\n",
      "  (Q un/di0ms0)\n",
      "  (Food pollo/ncms000)\n",
      "  y/cc\n",
      "  (Q una/di0fs0)\n",
      "  (Food ensalada/ncfs000)\n",
      "  ./Fp)\n",
      "(S quiero/sps00 (Q una/di0fs0) (Food paella/ncfs000) ./Fp)\n",
      "(S quiero/sps00 (Q un/di0ms0) (Food bocadillo/ncms000) ./Fp)\n",
      "(S quiero/sps00 (Q una/di0fs0) (Food pizza/ncfs000) ./Fp)\n",
      "(S ponme/da0mp0 (Q 3/Z) (Food sopas/ncfp000) ./Fp)\n",
      "(S quiero/sps00 (Q un/di0ms0) (Food filete/ncms000) ./Fp)\n",
      "(S\n",
      "  por/sps00\n",
      "  favor/ncms000\n",
      "  ,/Fc\n",
      "  me/pp1cs000\n",
      "  das/vmip3s0\n",
      "  (Q tres/dn0cp0)\n",
      "  (Food tacos/ncmp000)\n",
      "  ./Fp)\n",
      "(S\n",
      "  me/pp1cs000\n",
      "  puedes/vmip3s0\n",
      "  ayudar/vmn0000\n",
      "  con/sps00\n",
      "  (Q una/di0fs0)\n",
      "  (Food orden/ncfs000)\n",
      "  (Food de/sps00 papitas/np0000l y/cc camotes/ncmp000)\n",
      "  fritos/aq0mpp\n",
      "  ./Fp)\n",
      "(S\n",
      "  ayudeme/rg\n",
      "  con/sps00\n",
      "  (Q cuatro/dn0cp0)\n",
      "  (Food hamburguesas/ncmp000)\n",
      "  (Food con/sps00 cebollas/np0000l ,/Fc)\n",
      "  (Food mayonessa/np0000p)\n",
      "  y/cc\n",
      "  (Food mostaza/ncfs000)\n",
      "  ./Fp)\n",
      "(S\n",
      "  por/sps00\n",
      "  favor/da0mp0\n",
      "  (Q dos/dn0cp0)\n",
      "  (Food hotdogs/ncmp000)\n",
      "  con/sps00\n",
      "  salsa/vmn0000\n",
      "  (Food de/sps00 piña/np0000l)\n",
      "  ./Fp)\n",
      "(S quisiera/da0mp0 (Q 10/Z) (Food alitas/ncmp000) bbq/aq0mp0 ./Fp)\n",
      "(S\n",
      "  por/sps00\n",
      "  favor/ncms000\n",
      "  ,/Fc\n",
      "  me/pp1cs000\n",
      "  puedes/vmip3s0\n",
      "  ayudar/vmn0000\n",
      "  con/sps00\n",
      "  (Q una/di0fs0)\n",
      "  (Food ensalada/ncfs000)\n",
      "  (Food con/sps00 pollo/ncms000)\n",
      "  ./Fp)\n",
      "(S\n",
      "  (Q seis/dn0cp0)\n",
      "  (Food bolones/ncmp000)\n",
      "  (Food con/sps00 queso/np0000l)\n",
      "  ./Fp)\n",
      "(S quiero/da0mp0 (Q 12/Z) (Food tortillas/ncfp000) ./Fp)\n",
      "(S quiero/sps00 (Q una/di0fs0) (Food lasagna/ncfs000) ./Fp)\n",
      "(S\n",
      "  quiero/da0mp0\n",
      "  (Q 3/Z)\n",
      "  (Food bocadillos/ncmp000)\n",
      "  (Food de/sps00 anchoas/np0000l)\n",
      "  y/cc\n",
      "  (Q 2/Z)\n",
      "  pizzas/Fpt\n",
      "  ./Fp)\n",
      "(S\n",
      "  quisiera/sps00\n",
      "  pedir/vmn0000\n",
      "  (Q una/di0fs0)\n",
      "  (Food hamburguesa/ncfs000))\n"
     ]
    }
   ],
   "source": [
    "iob_tags = []\n",
    "for order in iob_tagged:\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unigram Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente sección se va a entrenar un unigram chunker con los POS tags y IOB tags que se obtuvieron con el RegexpParser. El objetivo es utilizar los pos tags para poder asignar un IOB tag a las palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.util import tree2conlltags,conlltags2tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('S', [('quiero', 'sps00'), Tree('Q', [('una', 'di0fs0')]), Tree('Food', [('pizza', 'ncfs000')]), Tree('Food', [('con', 'sps00'), ('jamón', 'ncms000')]), ('.', 'Fp')]),\n",
       " Tree('S', [('quiero', 'sps00'), Tree('Q', [('una', 'di0fs0')]), Tree('Food', [('pizza', 'ncfs000')]), Tree('Food', [('con', 'sps00'), ('queso', 'np0000l'), ('y', 'cc'), ('jamón', 'ncms000')]), ('.', 'Fp')])]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iob_tagged[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(iob_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnigramChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.UnigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se va a entrenar el **unigram chunker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  84.6%%\n",
      "    Precision:     84.6%%\n",
      "    Recall:        84.6%%\n",
      "    F-Measure:     84.6%%\n"
     ]
    }
   ],
   "source": [
    "train_sents = iob_tagged[:16]                     # Selecciono 17 de las 21 oraciones para el entrenamiento\n",
    "test_sents = iob_tagged[16:]                      # Reservo las 4 restantes para prueba y evaluación\n",
    "unigram_chunker = UnigramChunker(train_sents)     # Entreno el unigram chunker\n",
    "print(unigram_chunker.evaluate(test_sents))       # Evaluo el chunker con las oraciones de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El **unigram chunker** muestra un precission de 84.6%, lo que quiere decir que ha acertado el 84.6% de asignaciones de comida y cantidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a observar que aprendio el chunker mediante los tags que se asignaron a las oraciones de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Fc', 'O'), ('Fp', 'O'), ('Z', 'B-Q'), ('aq0mp0', 'O'), ('aq0mpp', 'O'), ('cc', 'O'), ('da0mp0', 'O'), ('di0fs0', 'B-Q'), ('di0ms0', 'B-Q'), ('dn0cp0', 'B-Q'), ('ncfp000', 'B-Food'), ('ncfs000', 'B-Food'), ('ncmp000', 'B-Food'), ('ncms000', 'I-Food'), ('np0000l', 'I-Food'), ('np0000p', 'B-Food'), ('pp1cs000', 'O'), ('rg', 'O'), ('sps00', 'O'), ('vmip3s0', 'O'), ('vmn0000', 'O')]\n"
     ]
    }
   ],
   "source": [
    "postags = sorted(set(pos for sent in train_sents\n",
    "                     for (word,pos) in sent.leaves()))\n",
    "print(unigram_chunker.tagger.tag(postags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y una nueva orden de comida, nunca antes vista por el chunker, para observar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Quiero/da0mp0\n",
      "  (Q tres/dn0cp0)\n",
      "  (Food donas/ncmp000)\n",
      "  con/sps00\n",
      "  (Food chocolate/ncms000)\n",
      "  y/cc\n",
      "  nueces/sn.e-SUJ)\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('Quiero tres donas con chocolate y nueces')\n",
    "tagged = hmm_tagger.tag(tokens)\n",
    "chunked = unigram_chunker.parse(tagged)\n",
    "print(chunked)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que en esta oración se reconoce correctamente la comida: donas, chocolate. Sin embargo, no ha reconocido nueces como comida. Y al ser unigram, no se toma en cuenta las preposiciones o las comas; como se había definido en la gramática inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se va a construir el diccionario de resultados utilizando todo el corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'comida': ['pizza', 'jamón'], 'cantidad': 1}\n",
      "1 {'comida': ['pizza', 'queso', 'jamón'], 'cantidad': 1}\n",
      "2 {'comida': ['hamburguesas', 'tocino'], 'cantidad': 4}\n",
      "3 {'comida': ['tortilla', 'cerveza'], 'cantidad': 1}\n",
      "4 {'comida': ['pollo', 'ensalada'], 'cantidad': 1}\n",
      "5 {'comida': ['paella'], 'cantidad': 1}\n",
      "6 {'comida': ['bocadillo'], 'cantidad': 1}\n",
      "7 {'comida': ['pizza'], 'cantidad': 1}\n",
      "8 {'comida': ['sopas'], 'cantidad': 3}\n",
      "9 {'comida': ['filete'], 'cantidad': 1}\n",
      "10 {'comida': ['favor', 'tacos'], 'cantidad': 3}\n",
      "11 {'comida': ['orden', 'papitas', 'camotes'], 'cantidad': 1}\n",
      "12 {'comida': ['hamburguesas', 'cebollas', 'mayonessa', 'mostaza'], 'cantidad': 4}\n",
      "13 {'comida': ['hotdogs', 'piña'], 'cantidad': 2}\n",
      "14 {'comida': ['alitas'], 'cantidad': 10}\n",
      "15 {'comida': ['favor', 'ensalada', 'pollo'], 'cantidad': 1}\n",
      "16 {'comida': ['bolones', 'queso'], 'cantidad': 6}\n",
      "17 {'comida': ['tortillas'], 'cantidad': 12}\n",
      "18 {'comida': ['lasagna'], 'cantidad': 1}\n",
      "19 {'comida': ['bocadillos', 'anchoas'], 'cantidad': 3}\n",
      "20 {'comida': ['hamburguesa'], 'cantidad': 1}\n"
     ]
    }
   ],
   "source": [
    "chunkParser = unigram_chunker\n",
    "iob_tagged = []\n",
    "orders_info = []\n",
    "for order in food_orders:\n",
    "    chunked = get_chunks(order)\n",
    "    #print(chunked)\n",
    "    iob_tagged.append(chunked)\n",
    "    food = get_food(chunked)\n",
    "    q = get_order_details(chunked, 'Q')\n",
    "    n = map_num(q)\n",
    "    orders_info.append(build_order_dict(food, n))\n",
    "    #print_order_info(order, food, n)\n",
    "for i in range(len(orders_info)):\n",
    "    print(i, orders_info[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se utiliza el **unigram chunker** sobre el corpus de entrenamiento, surgen las siuientes observaciones:\n",
    "* En la orden 10 y 15, dada la natraleza del unigram tagger, se ha perdido el contexto y se reconoce a la palabra *favor* como comida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tree('S', [('por', 'sps00'), Tree('Food', [('favor', 'ncms000')]), (',', 'Fc'), ('me', 'pp1cs000'), ('das', 'vmip3s0'), Tree('Q', [('tres', 'dn0cp0')]), Tree('Food', [('tacos', 'ncmp000')]), ('.', 'Fp')])\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iob_tagged[10].__repr__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede verificar que *favor* ha sido etiquetado como 'ncms00' y en los tags que aprendió el **unigram chunker** el IOB tag correspondiente es 'I-Food'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bigram Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora se va a definir y evaluar un **bigram chunker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramChunker(nltk.ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_sents):\n",
    "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
    "                      for sent in train_sents]\n",
    "        self.tagger = nltk.BigramTagger(train_data)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        pos_tags = [pos for (word,pos) in sentence]\n",
    "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
    "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
    "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
    "                     in zip(sentence, chunktags)]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  88.5%%\n",
      "    Precision:    100.0%%\n",
      "    Recall:        76.9%%\n",
      "    F-Measure:     87.0%%\n"
     ]
    }
   ],
   "source": [
    "train_sents = iob_tagged[:16]\n",
    "test_sents = iob_tagged[16:]\n",
    "bigram_chunker = BigramChunker(train_sents)\n",
    "print(bigram_chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver que el porcentaje de acierto en las asignaciones de comida y cantidad ha subido al 100%. Y en general el F-meassure también ha subido al 87%. Los aciertos totales representados por el accuracy también han subido al 88.5%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Quiero/da0mp0\n",
      "  (Q tres/dn0cp0)\n",
      "  (Food donas/ncmp000)\n",
      "  con/sps00\n",
      "  (Food chocolate/ncms000)\n",
      "  y/cc\n",
      "  nueces/sn.e-SUJ)\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('Quiero tres donas con chocolate y nueces')\n",
    "tagged = hmm_tagger.tag(tokens)\n",
    "chunked = bigram_chunker.parse(tagged)\n",
    "print(chunked)\n",
    "chunked.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 {'comida': ['pizza', 'jamón'], 'cantidad': 1}\n",
      "1 {'comida': ['pizza', 'queso', 'jamón'], 'cantidad': 1}\n",
      "2 {'comida': ['hamburguesas', 'tocino'], 'cantidad': 4}\n",
      "3 {'comida': ['tortilla', 'cerveza'], 'cantidad': 1}\n",
      "4 {'comida': ['pollo', 'ensalada'], 'cantidad': 1}\n",
      "5 {'comida': ['paella'], 'cantidad': 1}\n",
      "6 {'comida': ['bocadillo'], 'cantidad': 1}\n",
      "7 {'comida': ['pizza'], 'cantidad': 1}\n",
      "8 {'comida': ['sopas'], 'cantidad': 3}\n",
      "9 {'comida': ['filete'], 'cantidad': 1}\n",
      "10 {'comida': ['favor', 'tacos'], 'cantidad': 3}\n",
      "11 {'comida': ['orden', 'papitas', 'camotes'], 'cantidad': 1}\n",
      "12 {'comida': ['hamburguesas', 'cebollas', 'mayonessa', 'mostaza'], 'cantidad': 4}\n",
      "13 {'comida': ['hotdogs', 'piña'], 'cantidad': 2}\n",
      "14 {'comida': ['alitas'], 'cantidad': 10}\n",
      "15 {'comida': ['favor', 'ensalada', 'pollo'], 'cantidad': 1}\n",
      "16 {'comida': ['bolones', 'queso'], 'cantidad': 6}\n",
      "17 {'comida': ['tortillas'], 'cantidad': 12}\n",
      "18 {'comida': ['lasagna'], 'cantidad': 1}\n",
      "19 {'comida': ['bocadillos', 'anchoas'], 'cantidad': 3}\n",
      "20 {'comida': ['hamburguesa'], 'cantidad': 1}\n"
     ]
    }
   ],
   "source": [
    "chunkParser = unigram_chunker\n",
    "iob_tagged = []\n",
    "orders_info = []\n",
    "for order in food_orders:\n",
    "    chunked = get_chunks(order)\n",
    "    #print(chunked)\n",
    "    iob_tagged.append(chunked)\n",
    "    food = get_food(chunked)\n",
    "    q = get_order_details(chunked, 'Q')\n",
    "    n = map_num(q)\n",
    "    orders_info.append(build_order_dict(food, n))\n",
    "    #print_order_info(order, food, n)\n",
    "for i in range(len(orders_info)):\n",
    "    print(i, orders_info[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tag_util'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-96-1e4366e9f68a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtag_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackoff_tagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tag_util'"
     ]
    }
   ],
   "source": [
    "from tag_util import backoff_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n",
    "\n",
    "class TagChunker(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_chunks, tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "        train_sents = conll_tag_chunks(train_chunks)\n",
    "        self.tagger = backoff_tagger(train_sents, tagger_classes)\n",
    "    \n",
    "    def parse(self, tagged_sent):\n",
    "        if not tagged_sent: return None\n",
    "        (words, tags) = zip(*tagged_sent)\n",
    "        chunks = self.tagger.tag(tags)\n",
    "        wtc = zip(words, chunks)\n",
    "        return conlltags2tree([(w,t,c) for (w,(t,c)) in wtc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'backoff_tagger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-2a9c86d97ec2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miob_tagged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtest_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miob_tagged\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTagChunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-94-1ec68bcc784d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_chunks, tagger_classes)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_chunks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnigramTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBigramTagger\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtrain_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconll_tag_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_chunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackoff_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagged_sent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'backoff_tagger' is not defined"
     ]
    }
   ],
   "source": [
    "train_sents = iob_tagged[:16]\n",
    "test_sents = iob_tagged[16:]\n",
    "chunker = TagChunker(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-1a6b3a8e976c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmegam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmegam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig_megam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[1;34m(bin)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MEGAM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'megam.opt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam_686'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam_i686.opt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://www.umiacs.umd.edu/~hal/megam/index.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    695\u001b[0m     return next(\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n\u001b[0;32m    699\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    679\u001b[0m     \"\"\"\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n\u001b[0;32m    683\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "from nltk.classify import megam\n",
    "megam.config_megam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
    "\n",
    "    def __init__(self, train_sents):\n",
    "        train_set = []\n",
    "        for tagged_sent in train_sents:\n",
    "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
    "            history = []\n",
    "            for i, (word, tag) in enumerate(tagged_sent):\n",
    "                featureset = npchunk_features(untagged_sent, i, history)\n",
    "                train_set.append( (featureset, tag) )\n",
    "                history.append(tag)\n",
    "        self.classifier = nltk.MaxentClassifier.train(\n",
    "            train_set, algorithm='megam', trace=0)\n",
    "\n",
    "    def tag(self, sentence):\n",
    "        history = []\n",
    "        for i, word in enumerate(sentence):\n",
    "            featureset = npchunk_features(sentence, i, history)\n",
    "            tag = self.classifier.classify(featureset)\n",
    "            history.append(tag)\n",
    "        return zip(sentence, history)\n",
    "\n",
    "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
    "    def __init__(self, train_sents):\n",
    "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
    "                         nltk.chunk.tree2conlltags(sent)]\n",
    "                        for sent in train_sents]\n",
    "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        tagged_sents = self.tagger.tag(sentence)\n",
    "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
    "        return nltk.chunk.conlltags2tree(conlltags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-35dda2ced152>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"pos\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConsecutiveNPChunker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-08ea4f1ed261>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_sents)\u001b[0m\n\u001b[0;32m     26\u001b[0m                          nltk.chunk.tree2conlltags(sent)]\n\u001b[0;32m     27\u001b[0m                         for sent in train_sents]\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConsecutiveNPChunkTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-08ea4f1ed261>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_sents)\u001b[0m\n\u001b[0;32m     11\u001b[0m                 \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         self.classifier = nltk.MaxentClassifier.train(\n\u001b[1;32m---> 13\u001b[1;33m             train_set, algorithm='megam', trace=0)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(cls, train_toks, algorithm, trace, encoding, labels, gaussian_prior_sigma, **cutoffs)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'megam'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             return train_maxent_classifier_with_megam(\n\u001b[1;32m--> 341\u001b[1;33m                 \u001b[0mtrain_toks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgaussian_prior_sigma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcutoffs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m             )\n\u001b[0;32m    343\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tadm'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mtrain_maxent_classifier_with_megam\u001b[1;34m(train_toks, trace, encoding, labels, gaussian_prior_sigma, **kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'-multilabel'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# each possible la\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m     \u001b[0moptions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'multiclass'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainfile_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1489\u001b[1;33m     \u001b[0mstdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_megam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1490\u001b[0m     \u001b[1;31m# print './megam_i686.opt ', ' '.join(options)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1491\u001b[0m     \u001b[1;31m# Delete the training file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\megam.py\u001b[0m in \u001b[0;36mcall_megam\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'args should be a list of strings'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_megam_bin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mconfig_megam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;31m# Call megam via a subprocess\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\megam.py\u001b[0m in \u001b[0;36mconfig_megam\u001b[1;34m(bin)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'MEGAM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'megam.opt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam_686'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'megam_i686.opt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://www.umiacs.umd.edu/~hal/megam/index.html'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     )\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    695\u001b[0m     return next(\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n\u001b[0;32m    699\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    679\u001b[0m     \"\"\"\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n\u001b[0;32m    683\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    637\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'\\n\\n  For more information on %s, see:\\n    <%s>'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the megam file!\nUse software specific configuration paramaters or set the MEGAM environment variable.\n\n  For more information on megam, see:\n    <http://www.umiacs.umd.edu/~hal/megam/index.html>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "def npchunk_features(sentence, i, history):\n",
    "    word, pos = sentence[i]\n",
    "    return {\"pos\": pos}\n",
    "chunker = ConsecutiveNPChunker(train_sents)\n",
    "print(chunker.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
